{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a6c0b-796a-4618-b3fe-c04760eb863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, json, subprocess, sys\n",
    "import pandas as pd\n",
    "import importlib.util as il\n",
    "\n",
    "if None in [il.find_spec('python-ulid'), il.find_spec('pyperclip')]:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'python-ulid']);\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pyperclip']);\n",
    "    \n",
    "from ulid import ULID\n",
    "import pyperclip\n",
    "\n",
    "def gen_ulid():\n",
    "    return str(ULID.from_timestamp(time.time()))\n",
    "\n",
    "def convert_coord(c):\n",
    "    c = str(c)\n",
    "    j = len(c) - 6\n",
    "    d = int(c[0:2 + j])\n",
    "    m = int(c[2 + j:4 + j])\n",
    "    s = float(c[4 + j:6 + j] + '.' + c[6 + j:])\n",
    "    q = 1 if j == 0 else -1\n",
    "    coord = round(q * (d + m / 60 + s / 3600), 6)\n",
    "    \n",
    "    return coord\n",
    "\n",
    "def pprint(dict):\n",
    "    print(json.dumps(dict, indent=2))\n",
    "\n",
    "def comma_followed_by_number(s):\n",
    "    for i, char in enumerate(s[:-1]):\n",
    "        if char == ',' and s[i+1].isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_table_section_from_file(section_header, filename, offset=0):\n",
    "    offset *= 3\n",
    "    section_header = '******* ' + section_header + ' *******'\n",
    "\n",
    "    downloads_folder = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    with open(os.path.join(downloads_folder, filename), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    extracted_lines = []\n",
    "    inside_section = False\n",
    "    end_marker_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if section_header in line:\n",
    "            inside_section = True\n",
    "            extracted_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        if inside_section:\n",
    "            if end_marker_count > offset:\n",
    "                extracted_lines.append(line)\n",
    "            # Count lines that are mostly dashes\n",
    "            if line.strip().startswith('---'):\n",
    "                end_marker_count += 1\n",
    "                if end_marker_count >= 3 + offset:\n",
    "                    break\n",
    "\n",
    "    return \"\".join(extracted_lines)\n",
    "\n",
    "def remove_dash_lines(text):\n",
    "    cleaned_lines = [\n",
    "        line for line in text.splitlines()\n",
    "        if not line.strip().startswith(\"---\")\n",
    "    ]\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def convert_pipe_text_to_csv(multi_line_text):\n",
    "    csv_lines = []\n",
    "    for line in multi_line_text.splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        if '|' not in line:\n",
    "            continue\n",
    "        \n",
    "        fields = [field.strip() for field in line.strip('|').split('|')]\n",
    "        csv_line = '|'.join(fields)\n",
    "        csv_lines.append(csv_line)\n",
    "\n",
    "    return '\\n'.join(csv_lines)\n",
    "\n",
    "def csv_text_to_dataframe(csv_text):\n",
    "    lines = [line.strip() for line in csv_text.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    headers = [h.strip() for h in lines[0].split('|')]\n",
    "    \n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        fields = [f.strip() for f in line.split('|')]\n",
    "        data.append(fields)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    return df\n",
    "\n",
    "def read_adaptation_section(section_header, filename, offset=0):\n",
    "    text = extract_table_section_from_file(section_header, filename, offset)\n",
    "    text = remove_dash_lines(text)\n",
    "    text = convert_pipe_text_to_csv(text)\n",
    "    \n",
    "    return csv_text_to_dataframe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634ce9f-31fc-4188-8389-67b56cba9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratchpad Processing\n",
    "df = read_adaptation_section('FIX', 'adapt.txt', offset=0)\n",
    "\n",
    "df = df[(df['SIM Only'] == 'N')]\n",
    "df = df[['Name', 'Short Name', 'Description']]\n",
    "\n",
    "sp = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    name = row['Name']\n",
    "    short_name = row['Short Name']\n",
    "    desc = row['Description']\n",
    "\n",
    "    if 'DUMMY' in desc:\n",
    "        continue\n",
    "\n",
    "    if ',' in desc:\n",
    "        if not comma_followed_by_number(desc):\n",
    "            continue\n",
    "    \n",
    "    if len(name) == 3:\n",
    "        sp[name] = desc\n",
    "    elif len(short_name) == 3:\n",
    "        sp[short_name] = desc\n",
    "        \n",
    "# print(pprint(sp))\n",
    "\n",
    "df = read_adaptation_section('INTERFAC_AHO', 'adapt.txt', offset=0)\n",
    "fixes = sorted(df['Exit Fix'].unique().tolist())\n",
    "\n",
    "fix_pattern = {}\n",
    "for fix in fixes:\n",
    "    if fix in sp:\n",
    "        print('fix_pattern[\\'' + fix + '\\'] = \\'' + sp[fix] + '\\'')\n",
    "print()\n",
    "\n",
    "df = read_adaptation_section('FIXPAIRS', 'adapt.txt', offset=0)\n",
    "fixes2 = sorted(df['Exit Fix'].unique().tolist())\n",
    "\n",
    "for fix in fixes2:\n",
    "    if fix in fix_pattern:\n",
    "        continue\n",
    "    \n",
    "    if fix in sp:\n",
    "        print('fix_pattern[\\'' + fix + '\\'] = \\'' + sp[fix] + '\\'')\n",
    "\n",
    "fix_pattern = dict(sorted(fix_pattern.items(), key=lambda item: (0 if '#' in item[1] else 1, item[1])))\n",
    "pprint(fix_pattern)\n",
    "\n",
    "scratchpads = []\n",
    "\n",
    "for s in fix_pattern:\n",
    "    p = {}\n",
    "    p['id'] = gen_ulid()\n",
    "    p['airportIds'] = []\n",
    "    p['searchPattern'] = fix_pattern[s]\n",
    "\n",
    "    if 'AOA' in p['searchPattern']:\n",
    "        idx = p['searchPattern'].index('AOA ')\n",
    "        p['minAltitude'] = int(p['searchPattern'][idx + 4:])\n",
    "        p['searchPattern'] = p['searchPattern'][0:idx - 1]\n",
    "    elif 'AOB' in p['searchPattern']:\n",
    "        idx = p['searchPattern'].index('AOB ')\n",
    "        p['maxAltitude'] = int(p['searchPattern'][idx + 4:])\n",
    "        p['searchPattern'] = p['searchPattern'][0:idx - 1]\n",
    "    \n",
    "    p['template'] = s\n",
    "    scratchpads.append(p)\n",
    "\n",
    "scratchpads = sorted(scratchpads, key=lambda x: (not '#' in x['searchPattern'], x['searchPattern']))\n",
    "\n",
    "pyperclip.copy(scratchpads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311c2d5-8072-4f6e-83ba-88159b7aafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise \n",
    "# MIA Scratchpad Data\n",
    "fix_pattern['AAR'] = 'AARPS'\n",
    "fix_pattern['AGE'] = 'AGERS'\n",
    "fix_pattern['ALN'] = 'ALTNN'\n",
    "fix_pattern['ALT'] = 'ALTNN#'\n",
    "fix_pattern['BE1'] = 'BEECH AOA 101'\n",
    "fix_pattern['BEE'] = 'BEECH AOB 100'\n",
    "fix_pattern['BNC'] = 'BNICE'\n",
    "fix_pattern['BNG'] = 'BNGOS#'\n",
    "fix_pattern['BNI'] = 'BNICE#'\n",
    "fix_pattern['BNO'] = 'BNGOS'\n",
    "fix_pattern['DOL'] = 'DORRL'\n",
    "fix_pattern['DRL'] = 'DORRL#'\n",
    "fix_pattern['FEA'] = 'FEALX#'\n",
    "fix_pattern['FEL'] = 'FEALX'\n",
    "fix_pattern['FLG'] = 'FLMGO AOB 410'\n",
    "fix_pattern['FLM'] = 'FLMGO# AOB 410'\n",
    "fix_pattern['FM2'] = 'FLMGO# AOA 411'\n",
    "fix_pattern['FM3'] = 'FLMGO AOA 411'\n",
    "fix_pattern['FOL'] = 'FOLZZ#'\n",
    "fix_pattern['FOZ'] = 'FOLZZ'\n",
    "fix_pattern['FR2'] = 'FRSBE# AOA 411'\n",
    "fix_pattern['FR3'] = 'FRSBE AOA 411'\n",
    "fix_pattern['FRB'] = 'FRSBE AOB 410'\n",
    "fix_pattern['FRS'] = 'FRSBE# AOB 410'\n",
    "fix_pattern['GAB'] = 'GABOW#'\n",
    "fix_pattern['GAO'] = 'GABOW'\n",
    "fix_pattern['GLA'] = 'GLADZ'\n",
    "fix_pattern['GV1'] = 'GWAVA'\n",
    "fix_pattern['GWA'] = 'GWAVA#'\n",
    "fix_pattern['HRC'] = 'HROCK'\n",
    "fix_pattern['HRO'] = 'HROCK#'\n",
    "fix_pattern['HUC'] = 'HURCN'\n",
    "fix_pattern['HUR'] = 'HURCN#'\n",
    "fix_pattern['HUS'] = 'HUSIL'\n",
    "fix_pattern['KET'] = 'KETLL'\n",
    "fix_pattern['KLA'] = 'KLADA'\n",
    "fix_pattern['LI6'] = 'LIFRR AOB 060'\n",
    "fix_pattern['LI7'] = 'LIFFR AOA 061'\n",
    "fix_pattern['MAN'] = 'MAYNR'\n",
    "fix_pattern['MAY'] = 'MAYNR#'\n",
    "fix_pattern['MEL'] = 'MELLZ'\n",
    "fix_pattern['NNG'] = 'NNOCE# URSUS'\n",
    "fix_pattern['NNM'] = 'NNOCE# FUNDI'\n",
    "fix_pattern['PB3'] = 'T208'\n",
    "fix_pattern['REG'] = 'REGAE'\n",
    "fix_pattern['SNA'] = 'SNAPR'\n",
    "fix_pattern['ZFX'] = 'ZFP FLL'\n",
    "fix_pattern['ZFZ'] = 'ZFP MIA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec07de-77b9-4206-8003-21682c32ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise \n",
    "# TPA Scratchpad Data\n",
    "scratchpads = []\n",
    "\n",
    "sr = {\n",
    "    'A': ['LAL', 'YOJIX', 'V441', 'T336', 'T343'],\n",
    "    'B': ['DORMR#', 'TIDES# CIGAR', 'TIDES# FROOT', 'DORMR', 'SYKES', 'CIGAR', 'FROOT'],\n",
    "    'D': ['DARBS', 'V97 AOB 080', 'SZW'],\n",
    "    'F': ['GANDY#', 'GANDY', 'PAIRS', 'MURDO', 'SABEE', 'SRQ', 'V579', 'RSW', 'CEXAN', 'CHARO', 'VIOLA'],\n",
    "    'H': ['CROWD#', 'CROWD', 'HALLR', 'V509', 'FAZES'],\n",
    "    'M': ['PRICY#', 'MINEE#', 'MINEE', 'PRICY'],\n",
    "    'N': ['BAYPO#', 'BAYPO', 'CAMJO', 'WILON', 'NITTS AOA 100', 'V441', 'OCF', 'GNV'],\n",
    "    'R': ['KNEED', 'V152', 'T210', 'ORL', 'VARZE'],\n",
    "    'U': ['ENDED#', 'TIDES# MOMIE', 'TIDES# CAMJO', 'ENDED', 'LACEN', 'MOMIE', 'T495', 'T489', 'ATTAK', 'CTY'], \n",
    "}\n",
    "\n",
    "for k, v in sr.items():\n",
    "    for pattern in v:\n",
    "        s = {}\n",
    "        s['id'] = gen_ulid()\n",
    "        s['airportIds'] = []\n",
    "        s['searchPattern'] = pattern\n",
    "        s['template'] = k + '###'\n",
    "        scratchpads.append(s)\n",
    "\n",
    "pyperclip.copy(json.dumps(scratchpads, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
